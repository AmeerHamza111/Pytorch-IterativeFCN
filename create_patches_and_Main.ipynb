{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "create_patches_and_Main.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lTjJqrezvcQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iyCHkxyLz7r2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install SimpleITK\n",
        "!pip install medpy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GuRrWEobEMCl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "import os\n",
        "import numpy as np\n",
        "import SimpleITK as sitk\n",
        "from scipy import ndimage\n",
        "#pre-calculate of weight of masks\n",
        "\n",
        "def compute_distance_weight_matrix(mask, alpha=1, beta=8, omega=6):\n",
        "    mask = np.asarray(mask)\n",
        "    distance_to_border = ndimage.distance_transform_edt(mask > 0) + ndimage.distance_transform_edt(mask == 0)    \n",
        "    weights = alpha + beta*np.exp(-(distance_to_border**2/omega**2))\n",
        "    return np.asarray(weights, dtype='float32')\n",
        "\n",
        "mask_path =  './drive/My Drive/isotropic_dataset/test/seg'\n",
        "weight_path = './drive/My Drive/isotropic_dataset/test/weight'\n",
        "\n",
        "for f in [f for f in os.listdir(mask_path) if f.endswith('.mhd')]:\n",
        "  seg_mask = sitk.GetArrayFromImage(sitk.ReadImage(os.path.join(mask_path,f)))\n",
        "  weight = compute_distance_weight_matrix(seg_mask)\n",
        "  sitk.WriteImage(sitk.GetImageFromArray(weight), os.path.join(weight_path, f.split('_')[0]+'_weight.nrrd'), True)\n",
        "  print(f)\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajnnvLVl0Bdd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Created on Tue Sep 17 08:55:35 2019\n",
        "\n",
        "@author: Gabriel Hsu\n",
        "\n",
        "ref:https://www.kaggle.com/ori226/data-augmentation-with-elastic-deformations\n",
        "\n",
        "\"\"\"\n",
        "from __future__ import print_function, division\n",
        "import os \n",
        "from random import randint\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy import ndimage\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import SimpleITK as sitk\n",
        "\n",
        "from data_augmentation import elastic_transform, gaussian_blur, gaussian_noise, crop_z\n",
        "\n",
        "\"\"\"\n",
        "The dataset of MICCAI 2014 Spine Challenge\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "#%% Build the dataset \n",
        "class CSI_Dataset(Dataset):\n",
        "    \"\"\"xVertSeg Dataset\"\"\"\n",
        "    \n",
        "    def __init__(self, dataset_path, subset='train', linear_att=1.0, offset=1000.0):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            path_dataset(string): Root path to the whole dataset\n",
        "            subset(string): 'train' or 'test' depend on which subset\n",
        "        \"\"\"\n",
        "        self.idx = 1\n",
        "        \n",
        "        self.dataset_path = dataset_path\n",
        "        self.subset = subset\n",
        "        self.linear_att = linear_att\n",
        "        self.offset = offset\n",
        "        \n",
        "        \n",
        "        self.img_path = os.path.join(dataset_path, subset, 'img')\n",
        "        self.mask_path = os.path.join(dataset_path, subset, 'seg')\n",
        "        self.weight_path = os.path.join(dataset_path, subset, 'weight')\n",
        "        \n",
        "        self.img_names =  [f for f in os.listdir(self.img_path) if f.endswith('.mhd')]\n",
        "#        self.mask_names = [f for f in os.listdir(self.mask_path) if f.endswith('.mhd')]\n",
        "\n",
        "     \n",
        "    def __len__(self):\n",
        "        return len(self.img_names)\n",
        "    \n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "    \n",
        "        img_name =  self.img_names[idx]\n",
        "        mask_name = self.img_names[idx].split('.')[0]+'_label.mhd'\n",
        "        weight_name = self.img_names[idx].split('.')[0]+'_weight.nrrd'\n",
        "        \n",
        "        img_file = os.path.join(self.img_path,  img_name)\n",
        "        mask_file = os.path.join(self.mask_path, mask_name)\n",
        "        weight_file = os.path.join(self.weight_path, weight_name)\n",
        "        \n",
        "        img = sitk.GetArrayFromImage(sitk.ReadImage(img_file))\n",
        "        mask = sitk.GetArrayFromImage(sitk.ReadImage(mask_file))\n",
        "        weight = sitk.GetArrayFromImage(sitk.ReadImage(weight_file))\n",
        "        \n",
        "        #z, y, x\n",
        "        #print(img.shape)\n",
        "        #print(mask.shape)\n",
        "        #linear transformation from 12bit reconstruction img to HU unit\n",
        "        #depend on the original data (CSI data value is from 0 ~ 4095)\n",
        "        #img = img * self.linear_att - self.offset\n",
        "        \n",
        "        #image standardize\n",
        "        m = np.mean(img)\n",
        "        s = np.std(img)\n",
        "        img-=m\n",
        "        img/=s\n",
        "        \n",
        "        \n",
        "        img_patch, ins_patch, gt_patch, weight_patch, c_label = extract_random_patch(img, \n",
        "                                                              mask, weight, self.idx)\n",
        "        #print(np.mean(img_patch), np.std(img_patch))\n",
        "\n",
        "        self.idx+=1\n",
        "            \n",
        "        return img_patch, ins_patch, gt_patch, weight_patch, c_label\n",
        "        \n",
        "#%% Compute weight distance for loss function\n",
        "def compute_distance_weight_matrix(mask, alpha=1, beta=8, omega=6):\n",
        "    mask = np.asarray(mask)\n",
        "    distance_to_border = ndimage.distance_transform_edt(mask > 0) + ndimage.distance_transform_edt(mask == 0)    \n",
        "    weights = alpha + beta*np.exp(-(distance_to_border**2/omega**2))\n",
        "    return np.asarray(weights, dtype='float32')\n",
        "    \n",
        "    \n",
        "#%% Extract the 128*128*128 patch\n",
        "def extract_random_patch(img, mask, weight, i, patch_size=128):\n",
        "    \n",
        "    \n",
        "    #list available vertebrae\n",
        "    verts = np.unique(mask)\n",
        "#    print('mask values:', verts)\n",
        "    chosen_vert = verts[randint(1, len(verts)-1)]\n",
        "#    print('chosen_vert:', chosen_vert)\n",
        "    \n",
        "    #create corresponde instance memory and ground truth\n",
        "    ins_memory = np.copy(mask)\n",
        "    ins_memory[ins_memory <= chosen_vert] = 0\n",
        "    ins_memory[ins_memory > 0] = 1\n",
        "#    print(np.unique(ins_memory))\n",
        "    \n",
        "    gt = np.copy(mask)\n",
        "    gt[gt != chosen_vert] = 0\n",
        "    gt[gt > 0] = 1\n",
        "#    print(np.unique(gt))\n",
        "\n",
        "    flag_empty = False\n",
        "    \n",
        "    if not i%6:\n",
        "        #print(i, ' empty mask')\n",
        "        patch_center = [np.random.randint(0, s) for s in img.shape]\n",
        "        lower = [0, 0, 0]\n",
        "        \n",
        "        upper = [img.shape[0], img.shape[1], img.shape[2]]\n",
        "        x = patch_center[2]\n",
        "        y = patch_center[1]\n",
        "        z = patch_center[0]\n",
        "        \n",
        "        #for ins\n",
        "        gt = np.copy(mask)\n",
        "        \n",
        "        flag_empty = True\n",
        "        \n",
        "    else:\n",
        "        print(i, ' normal sample')\n",
        "        indices = np.nonzero(mask == chosen_vert)\n",
        "        lower = [np.min(i) for i in indices]\n",
        "        upper = [np.max(i) for i in indices]\n",
        "        #random center of patch\n",
        "        x = randint(lower[2], upper[2])\n",
        "        y = randint(lower[1], upper[1])\n",
        "        z = randint(lower[0], upper[0])\n",
        "    \n",
        "    #extract the patch and padding\n",
        "    x_low = int(max(x-patch_size/2, 0))\n",
        "    x_up = int(min(x+patch_size/2,  img.shape[2]))\n",
        "    \n",
        "    y_low = int(max(y-patch_size/2, 0))\n",
        "    y_up = int(min(y+patch_size/2,  img.shape[1]))\n",
        "    \n",
        "    z_low = int(max(z-patch_size/2, 0))\n",
        "    z_up = int(min(z+patch_size/2,  img.shape[0]))\n",
        "    \n",
        "    x_pad, y_pad, z_pad = np.zeros(2), np.zeros(2), np.zeros(2)\n",
        "    \n",
        "    img_patch = img[z_low:z_up, y_low:y_up, x_low:x_up]\n",
        "    ins_patch = ins_memory[z_low:z_up, y_low:y_up, x_low:x_up]\n",
        "    gt_patch = gt[z_low:z_up, y_low:y_up, x_low:x_up]\n",
        "    \n",
        "    weight_patch = weight[z_low:z_up, y_low:y_up, x_low:x_up]\n",
        "    \n",
        "\n",
        "    #paddding the patch to 128*128*128\n",
        "    if x_low == 0:\n",
        "      x_pad[0] = int(patch_size - img_patch.shape[2]) \n",
        "    elif x_up == img.shape[2]:\n",
        "      x_pad[1] = int(patch_size - img_patch.shape[2]) \n",
        "      \n",
        "    if y_low == 0:\n",
        "      y_pad[0] = int(patch_size - img_patch.shape[1]) \n",
        "    elif y_up == img.shape[1]:\n",
        "      y_pad[1] = int(patch_size - img_patch.shape[1]) \n",
        "      \n",
        "    if z_low == 0:\n",
        "      z_pad[0] = int(patch_size - img_patch.shape[0]) \n",
        "    elif z_up == img.shape[0]:\n",
        "      z_pad[1] = int(patch_size - img_patch.shape[0]) \n",
        "   \n",
        "    x_pad = x_pad.astype(int)\n",
        "    y_pad = y_pad.astype(int)\n",
        "    z_pad = z_pad.astype(int)\n",
        "    \n",
        "    img_patch = np.pad(img_patch, ((z_pad[0], z_pad[1]), (y_pad[0], y_pad[1]), (x_pad[0], x_pad[1])), 'constant', constant_value=img.min())\n",
        "    ins_patch = np.pad(ins_patch, ((z_pad[0], z_pad[1]), (y_pad[0], y_pad[1]), (x_pad[0], x_pad[1])), 'constant', constant_values=ins_memory.min())\n",
        "    gt_patch = np.pad(gt_patch,   ((z_pad[0], z_pad[1]), (y_pad[0], y_pad[1]), (x_pad[0], x_pad[1])), 'constant', constant_values=gt.min())\n",
        "    weight_patch = np.pad(weight_patch, ((z_pad[0], z_pad[1]), (y_pad[0], y_pad[1]), (x_pad[0], x_pad[1])), 'constant', constant_values=weight.min())\n",
        "    \n",
        "    \n",
        "    if flag_empty:\n",
        "      #print('1/6 patches produced')\n",
        "      ins_patch = gt_patch\n",
        "      gt_patch = np.zeros_like(ins_patch)\n",
        "      weight = np.ones_like(ins_patch)\n",
        "    \n",
        "    \n",
        "    #give the label of completeness(partial or complete)\n",
        "    vol = np.count_nonzero(gt == 1)\n",
        "    sample_vol = np.count_nonzero(gt_patch == 1 )\n",
        "    \n",
        "    #print('visible volume:{:.6f}'.format(float(sample_vol/(vol+0.0001))))\n",
        "    c_label = 0 if float(sample_vol/(vol+0.0001)) < 0.98 else 1\n",
        "    \n",
        "    #Randomly Data Augmentation\n",
        "    # 50% chance elastic deformation\n",
        "    aug = randint(0,3)\n",
        "    \n",
        "    if aug==0 and not flag_empty:\n",
        "        #print('elastic deform')\n",
        "        img_patch, gt_patch, ins_patch, weight_patch = elastic_transform(img_patch, gt_patch, ins_patch, weight_patch, alpha=300, sigma=8)\n",
        "    # 50% chance gaussian blur\n",
        "    if aug==1 and not flag_empty:\n",
        "        #print('gaussian blur')\n",
        "        img_patch = gaussian_blur(img_patch)\n",
        "    # 50% chance gaussian noise\n",
        "    if aug==2 and not flag_empty:\n",
        "        #print('gaussian noise')\n",
        "        img_patch = gaussian_noise(img_patch)\n",
        "    \"\"\"\n",
        "    # 20% chance random crop \n",
        "    if np.random.rand() <= 0.5:\n",
        "#        print('random crop')\n",
        "        k = randint(0, 128)\n",
        "        img_patch, ins_patch, gt_patch = crop_z(img_patch, ins_patch, gt_patch, k)\n",
        "    \"\"\"    \n",
        "    \n",
        "    \n",
        "    img_patch = np.expand_dims(img_patch, axis=0)\n",
        "    ins_patch = np.expand_dims(ins_patch, axis=0)\n",
        "    gt_patch = np.expand_dims(gt_patch, axis=0)\n",
        "    weight_patch = np.expand_dims(weight_patch, axis=0)\n",
        "    c_label = np.expand_dims(c_label, axis=0)\n",
        "    \n",
        "    \n",
        "    return img_patch, ins_patch, gt_patch, weight_patch, c_label\n",
        "\n",
        "\n",
        "#%%% Test purpose\n",
        "data_root = './drive/My Drive/isotropic_dataset'\n",
        "train_set = CSI_Dataset(data_root, subset='train')\n",
        "test_set = CSI_Dataset(data_root, subset='test')\n",
        "\n",
        "dataloader_train = DataLoader(train_set, batch_size=1)\n",
        "dataloader_test = DataLoader(test_set, batch_size=1)\n",
        "\n",
        "\"\"\"\n",
        "#produce training patches\n",
        "training_sample = 5000\n",
        "c_train = []\n",
        "\n",
        "for i in range(training_sample):\n",
        "  img_patch, ins_patch, gt_patch, weight, c_label = next(iter(dataloader_train))\n",
        "\n",
        " \n",
        "  print(i, ' samples generated...')\n",
        "  \n",
        "  img_patch = torch.squeeze(img_patch)\n",
        "  ins_patch = torch.squeeze(ins_patch)\n",
        "  gt_patch = torch.squeeze(gt_patch)\n",
        "  weight = torch.squeeze(weight)\n",
        "  c_train.append(c_label.item())\n",
        "\n",
        "\n",
        "  sitk.WriteImage(sitk.GetImageFromArray(img_patch.numpy()), './drive/My Drive/patches/train/img/img_'+ str(i)+'.nrrd', True)\n",
        "  sitk.WriteImage(sitk.GetImageFromArray(gt_patch.numpy()), './drive/My Drive/patches/train/gt/gt_'+ str(i)+'.nrrd', True)\n",
        "  sitk.WriteImage(sitk.GetImageFromArray(ins_patch.numpy()), './drive/My Drive/patches/train/ins/ins_'+ str(i)+'.nrrd', True)\n",
        "  sitk.WriteImage(sitk.GetImageFromArray(weight.numpy()), './drive/My Drive/patches/train/weight/weight_'+ str(i)+'.nrrd', True)\n",
        "\n",
        "\n",
        "pd.Series(c_train).to_excel('./drive/My Drive/patches/train/train_label.xlsx')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#produce test patches\n",
        "test_sample = 1000\n",
        "c_test = []\n",
        "for i in range(test_sample):\n",
        "  img_patch, ins_patch, gt_patch, weight, c_label = next(iter(dataloader_test))\n",
        "\n",
        "  if not i % 1000:\n",
        "    print(i, ' samples generated...')\n",
        "\n",
        "\n",
        "  img_patch = torch.squeeze(img_patch)\n",
        "  ins_patch = torch.squeeze(ins_patch)\n",
        "  gt_patch = torch.squeeze(gt_patch)\n",
        "  weight = torch.squeeze(weight)\n",
        "  c_test.append(c_label.item())\n",
        "\n",
        "\n",
        "  sitk.WriteImage(sitk.GetImageFromArray(img_patch.numpy()), './drive/My Drive/patches/test/img/img_'+ str(i)+'.nrrd', True)\n",
        "  sitk.WriteImage(sitk.GetImageFromArray(gt_patch.numpy()), './drive/My Drive/patches/test/gt/gt_'+ str(i)+'.nrrd', True)\n",
        "  sitk.WriteImage(sitk.GetImageFromArray(ins_patch.numpy()), './drive/My Drive/patches/test/ins/ins_'+ str(i)+'.nrrd', True)\n",
        "  sitk.WriteImage(sitk.GetImageFromArray(weight.numpy()), './drive/My Drive/patches/test/weight/weight_'+ str(i)+'.nrrd', True)\n",
        "\n",
        "\n",
        "pd.Series(c_test).to_excel('./drive/My Drive/patches/test/test_label.xlsx')\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O95a1aStKHRI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Training and Evaluation\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Thu Sep 19 11:21:22 2019\n",
        "\n",
        "@author: Gabriel Hsu\n",
        "\"\"\"\n",
        "from __future__ import print_function, division\n",
        "import os\n",
        "import argparse\n",
        "import time\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.optim as optim\n",
        "\n",
        "from model import iterativeFCN\n",
        "#from dataset_simple import CSI_Dataset_Patched\n",
        "from metrics import DiceCoeff, ASSD\n",
        "\n",
        "import SimpleITK as sitk\n",
        "\n",
        "def seg_loss(pred, target, weight):\n",
        "    size = pred.shape[0]\n",
        "    FP = torch.sum(weight*(1-target)*pred)\n",
        "    FN = torch.sum(weight*(1-pred)*target)\n",
        "    return FP/size, FN/size\n",
        "    \n",
        "#%%\n",
        "def train_single(args, model, device, img_patch, ins_patch, gt_patch, weight, c_label, optimizer):\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "  \n",
        "    model.train()\n",
        "    correct = 0\n",
        "    \n",
        "    img_patch = img_patch.float()\n",
        "    ins_patch = ins_patch.float()\n",
        "    gt_patch = gt_patch.float()\n",
        "    weight = weight.float()\n",
        "    c_label = c_label.float()\n",
        "    \n",
        "    \n",
        "    #pick a random scan\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    #concatenate the img_patch and ins_patch\n",
        "    input_patch = torch.cat((img_patch, ins_patch), dim=1)\n",
        "    input_patch, gt_patch, weight, c_label = input_patch.to(device), gt_patch.to(device), weight.to(device), c_label.to(device)\n",
        "    S, C = model(input_patch.float())        \n",
        "    \n",
        "    \n",
        "    #Calculate DiceCoeff\n",
        "    pred = torch.round(S).detach()\n",
        "    train_dice_coef =  DiceCoeff(pred, gt_patch.detach())\n",
        "    \n",
        "\n",
        "    #compute the loss\n",
        "    lamda = 0.1\n",
        "    \n",
        "    #segloss \n",
        "    FP, FN = seg_loss(S, gt_patch, weight) \n",
        "    \n",
        "    s_loss = lamda*FP + FN\n",
        "    \n",
        "    c_loss = -1*c_label*torch.log(C)-(1-c_label)*torch.log(1-C)\n",
        "\n",
        "    print(s_loss.item(), c_loss.item())\n",
        "    \n",
        "    train_loss = s_loss + c_loss\n",
        "    \n",
        "    \n",
        "    \n",
        "    if C.round() == c_label:\n",
        "        correct = 1\n",
        "    \n",
        "    #optimize the parameters\n",
        "    train_loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return train_loss.item(), correct, train_dice_coef\n",
        "\n",
        "def test_single(args, model, device, img_patch, ins_patch, gt_patch, weight, c_label):\n",
        "    \n",
        "    torch.cuda.empty_cache()\n",
        "    \n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    \n",
        "    img_patch = img_patch.float()\n",
        "    ins_patch = ins_patch.float()\n",
        "    gt_patch = gt_patch.float()\n",
        "    weight = weight.float()\n",
        "    c_label = c_label.float()\n",
        "    \n",
        "    input_patch = torch.cat((img_patch, ins_patch), dim=1)\n",
        "    input_patch, gt_patch, weight, c_label = input_patch.to(device), gt_patch.to(device), weight.to(device), c_label.to(device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        S, C = model(input_patch.float())\n",
        "        \n",
        "    \"\"\"\n",
        "    pred = torch.squeeze(S.to('cpu'))\n",
        "    sitk.WriteImage(sitk.GetImageFromArray(pred.numpy()), './pred.nrrd', True)\n",
        "    \n",
        "    gtt = torch.squeeze(gt_patch.to('cpu'))\n",
        "    sitk.WriteImage(sitk.GetImageFromArray(gtt.numpy()), './gt.nrrd', True)\n",
        "    \"\"\"\n",
        "    \n",
        "    #Calculate DiceCoeff\n",
        "    pred = torch.round(S).detach()\n",
        "    test_dice_coef =  DiceCoeff(pred, gt_patch.detach())  \n",
        "    \n",
        "    #compute the loss\n",
        "    lamda = 0.1\n",
        "    \n",
        "    #segloss \n",
        "    FP, FN = seg_loss(S, gt_patch, weight) \n",
        "    \n",
        "    s_loss = lamda*FP + FN\n",
        "    \n",
        "    c_loss = -1*c_label*torch.log(C)-(1-c_label)*torch.log(1-C)\n",
        "    \n",
        "    print(s_loss.item(), c_loss.item())\n",
        "    \n",
        "    if C.round() == c_label:\n",
        "        correct = 1\n",
        "\n",
        "    test_loss = s_loss + c_loss\n",
        "    \n",
        "        \n",
        "    return test_loss.item(), correct, test_dice_coef\n",
        "    \n",
        "#%%Main\n",
        "if  __name__ == \"__main__\" :   \n",
        "    # Version of Pytorch\n",
        "    print(\"Pytorch Version:\", torch.__version__)\n",
        "    \n",
        "    # Training args\n",
        "    parser = argparse.ArgumentParser(description='Fully Convolutional Network')\n",
        "    parser.add_argument('--batch-size', type=int, default=1, metavar='N',\n",
        "                        help='input batch size for training (default: 64)')\n",
        "    parser.add_argument('--test-batch-size', type=int, default=1, metavar='N',\n",
        "                        help='input batch size for testing (default: 1000)')\n",
        "    parser.add_argument('--epochs', type=int, default=100, metavar='N',\n",
        "                        help='number of epochs to train (default: 10)')\n",
        "    parser.add_argument('--lr', type=float, default=0.001, metavar='LR',\n",
        "                        help='learning rate (default: 0.01)')\n",
        "    parser.add_argument('--momentum', type=float, default=0.99, metavar='M',\n",
        "                        help='SGD momentum (default: 0.5)')\n",
        "    parser.add_argument('--no-cuda', action='store_true', default=True,\n",
        "                        help='disables CUDA training')\n",
        "    parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
        "                        help='random seed (default: 1)')\n",
        "    parser.add_argument('--log-interval', type=int, default=1000, metavar='N',\n",
        "                        help='how many batches to wait before logging training status')\n",
        "    \n",
        "    parser.add_argument('--save-model', action='store_true', default=True,\n",
        "                        help='For Saving the current Model')\n",
        "    \n",
        "    args = parser.parse_known_args()[0]\n",
        "    #args = parser.parse_args()\n",
        "\n",
        "    # Use GPU if it is available\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    #data_root = './drive/My Drive/patches'\n",
        "    \n",
        "    \n",
        "    # Create FCN\n",
        "    model = iterativeFCN().to('cuda')\n",
        "    #model.load_state_dict(torch.load('./drive/My Drive/IterativeFCN_best_norm.pth'))\n",
        "     \n",
        "\n",
        "    \"\"\"\n",
        "    batch_size = args.batch_size\n",
        "    batch_size_valid = batch_size\n",
        "    \n",
        "    train_clabel = list(pd.read_excel(os.path.join(data_root, 'train/train_label.xlsx'))[0])   \n",
        "    test_clabel = list(pd.read_excel(os.path.join(data_root, 'test/test_label.xlsx'))[0])   \n",
        "\n",
        "    \n",
        "    train_set = CSI_Dataset_Patched(data_root, train_clabel, subset='train')\n",
        "    test_set = CSI_Dataset_Patched(data_root, test_clabel, subset='test')\n",
        "    \n",
        "    \"\"\"\n",
        "    train_loader = dataloader_train\n",
        "    test_loader = dataloader_test\n",
        "    \n",
        "#%%    \n",
        "    #optimizer\n",
        "    optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
        "    \n",
        "    train_loss = []\n",
        "    test_loss = []\n",
        "    train_acc = []\n",
        "    test_acc = []\n",
        "    train_dice = []\n",
        "    test_dice = []\n",
        "    best_test_dice = 0\n",
        "    \n",
        "    total_iteration = 20000\n",
        "    train_interval = 50\n",
        "    eval_interval =  10\n",
        "    \n",
        "    # Start Training\n",
        "    for epoch in range(int(total_iteration/train_interval)):\n",
        "        \n",
        "        start_time = time.time()\n",
        "        epoch_train_dice = []\n",
        "        epoch_test_dice = []\n",
        "        epoch_train_loss = []\n",
        "        epoch_test_loss = []\n",
        "        epoch_train_accuracy = 0.\n",
        "        epoch_test_accuracy = 0.\n",
        "        correct_train_count = 0\n",
        "        correct_test_count = 0\n",
        "        \n",
        "        #training process\n",
        "        for i in range(train_interval):\n",
        "            img_patch, ins_patch, gt_patch, weight, c_label = next(iter(train_loader))\n",
        "            t_loss, t_c, t_dice = train_single(args, model, device, img_patch, ins_patch, gt_patch, weight, c_label, optimizer)\n",
        "            epoch_train_loss.append(t_loss)\n",
        "            epoch_train_dice.append(t_dice)\n",
        "            correct_train_count+=t_c\n",
        "            \n",
        "        epoch_train_accuracy = correct_train_count/train_interval\n",
        "        avg_train_loss = sum(epoch_train_loss) / len(epoch_train_loss)\n",
        "        avg_train_dice = sum(epoch_train_dice) / len(epoch_train_dice)\n",
        "        \n",
        "        print('Train Epoch: {} \\t Loss: {:.6f}\\t acc: {:.6f}%\\t dice: {:.6f}%'.format(epoch\n",
        "              , avg_train_loss\n",
        "              , epoch_train_accuracy*100\n",
        "              , avg_train_dice*100))\n",
        "\n",
        "        \n",
        "        #validation process\n",
        "        for i in range(eval_interval):\n",
        "            img_patch, ins_patch, gt_patch, weight, c_label = next(iter(test_loader))\n",
        "            v_loss, v_c, v_dice = test_single(args, model, device, img_patch, ins_patch, gt_patch, weight, c_label)\n",
        "            epoch_test_loss.append(v_loss)\n",
        "            epoch_test_dice.append(v_dice)\n",
        "            correct_test_count+=v_c\n",
        "            \n",
        "        epoch_test_accuracy = correct_test_count/eval_interval\n",
        "        avg_test_loss = sum(epoch_test_loss) / len(epoch_test_loss)\n",
        "        avg_test_dice = sum(epoch_test_dice) / len(epoch_test_dice)\n",
        "        \n",
        "        \n",
        "        print('Validation Epoch: {} \\t Loss: {:.6f}\\t acc: {:.6f}%\\t dice: {:.6f}%'.format(epoch\n",
        "              , avg_test_loss\n",
        "              , epoch_test_accuracy*100\n",
        "              , avg_test_dice*100))\n",
        "        \n",
        "        if avg_test_dice > best_test_dice:\n",
        "            best_test_dice = avg_test_dice\n",
        "            print('--- Saving model at Avg Test Dice:{:.2f}%  ---'.format(avg_test_dice))\n",
        "            torch.save(model.state_dict(),'./drive/My Drive/IterativeFCN_best_norm.pth')\n",
        "        \n",
        "        print('-------------------------------------------------------')\n",
        "        \n",
        "        train_loss.append(epoch_train_loss)\n",
        "        test_loss.append(epoch_test_loss)\n",
        "        train_acc.append(epoch_train_accuracy)\n",
        "        test_acc.append(epoch_test_accuracy)\n",
        "        \n",
        "\n",
        "        print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
        "\n",
        "        \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GtEDTSx2UMC0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"training:\", len(train_loss))\n",
        "print(\"validation:\", len(test_loss))\n",
        "x = list(range(1, len(train_loss)))\n",
        "#plot train/validation loss versus epoch\n",
        "plt.figure()\n",
        "plt.title(\"Train/Validation Loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Total Loss\")\n",
        "plt.plot(x, train_loss,label=\"train loss\")\n",
        "plt.plot(x, test_loss, color='red', label=\"validation loss\")\n",
        "plt.legend(loc='upper right')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "#plot train/validation loss versus epoch\n",
        "plt.figure()\n",
        "plt.title(\"Train/Validation Accuracy\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.plot(x, train_acc,label=\"train acc\")\n",
        "plt.plot(x, test_acc, color='red', label=\"validation acc\")\n",
        "plt.legend(loc='upper right')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVSy6-75UNVp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}